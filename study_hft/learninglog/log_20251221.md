- [x] https://huggingface.co/docs/transformers/v5.0.0rc1/index
    - model-definition framework for ML models in text, computer vision, audio, video, and multimodal model, for both inference and training
    - centralizes the model definition
    - if a model definition is supported, it will be compatible with the majority of training frameworks and adjacent modeling libraries
    - Models Timeline - https://huggingface.co/docs/transformers/main/models_timeline
    - Some of the main features:
        - Pipeline
            - inference class for many machine learning tasks
                - like text generation, image segmentation, automatic speech recognition, document question answering, and more.
        - Trainer
            - to train or fine-tune a PyTorch model
            - supports features such as mixed precision, torch.compile, and FlashAttention for training and distributed training for PyTorch models.
        - generate
            - Fast text generation with large language models (LLMs) and vision language models (VLMs), including support for streaming and multiple decoding strategies.
- [x] https://huggingface.co/docs/transformers/v5.0.0rc1/philosophy
    - PyTorch-first library
    - Three core classes are required for each model
        - configuration
        - models
        - preprocessing class
    - Tokenizers handle NLP
    - image processors handle images
    - video processors handle videos
    - feature extractors handle audio
    - processors handle multimodal inputs
    - Main classes
        - Configuration classes
            - store the hyperparameters required to build a model
        - Model classes
            - PyTorch models (torch.nn.Module), wrapped by at least a PreTrainedModel
        - Preprocessing classes
            - convert the raw data into a format accepted by the model
                - A tokenizer stores the vocabulary for each model and provides methods for encoding and decoding strings in a list of token embedding indices
                - Image processors preprocess vision inputs
                - video processors preprocess videos inputs
                - feature extractors preprocess audio inputs
                - processors preprocess multimodal inputs
    - from_pretrained()
        - instantiate a model, configuration, and preprocessing class from a pretrained version
    - save_pretrained()
        - save a model, configuration, and preprocessing class locally so that it can be reloaded
    - push_to_hub()
        - share a model, configuration, and a preprocessing class to the Hub, so it is easily accessible to everyone
- [x] https://huggingface.co/docs/transformers/v5.0.0rc1/quicktour
    - Each pretrained model inherits from three base classes.
        - PreTrainedConfig
            - A file that specifies a models attributes such as the number of attention heads or vocabulary size.
        - PreTrainedModel
            - A model (or architecture) defined by the model attributes from the configuration file. A pretrained model only returns the raw hidden states.
        - Preprocessor
            - A class for converting raw inputs (text, images, audio, multimodal) into numerical inputs to the model.
    - it is recommended to use the AutoClass API to load models and preprocessors
    - Use from_pretrained() to load the weights and configuration file from the Hub or local directories into the model and preprocessor class
    - model load parameters
        - device_map="auto"
            - automatically allocates the model weights to your fastest device first.
        - dtype="auto"
            - directly initializes the model weights in the data type theyâ€™re stored in
    - For inference, pass the tokenized inputs to generate() to generate text
        - Decode the token ids back into text with batch_decode()
    - Trainer
        - abstracts away a lot of the boilerplate
            - You only need a model, dataset, a preprocessor, and a data collator to build batches of data from the dataset.
        - Use the `TrainingArguments` class to customize the training process
            - Experiment with training hyperparameters and features
        - Load a model, tokenizer, and dataset for training.
        - tokenize dataset
        - Load a data collator to create batches of data
    - Quantization - Reduce memory and storage requirements and speed up inference by representing weights with fewer bits.
