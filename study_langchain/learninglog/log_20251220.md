- langchain
    - [x] https://docs.langchain.com/oss/python/langchain/agents
        - Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.
        - An agent runs in a loop until a stop condition is met
        - follows the ReAct (“Reasoning + Acting”) pattern
        - core components
            1. model
                - the reasoning engine of an agent
                - static models
                    - configured once when creating the agent and remain unchanged
                - dynamic models
                    - selected at runtime based on the current state and context.
                        - If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.
            2. tools
                - give agents the ability to take actions
                - To customize how tool errors are handled, use the `@wrap_tool_call` decorator
            3. system prompt
                - shape how an agent approaches tasks
                - dynamic system prompt
                    - `@dynamic_prompt` middleware can be used to modify the system prompt based on runtime context or agent state
        - LangChain provides strategies for structured output via the `response_format` parameter 
            - `ToolStrategy`
            - `ProviderStrategy`
            - simply passing a schema (e.g., `response_format=ContactInfo`) is no longer supported
        - Use middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.
        - Use the `state_schema` parameter as a shortcut to define custom state that is only used in tools.
            - As of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported.
        - To show intermediate progress, agent can stream back messages as they occur.
    - [x] https://docs.langchain.com/oss/python/langchain/models
        - many models support:
            - Tool calling
                - calling external tools (like databases queries or API calls) and use results in their responses.
                - some model providers offer built-in tools
            - Structured output
                - constraining the model’s response to follow a defined format.
            - Multimodality
                - process and return data other than text, such as images, audio, and video.
                - non-textual data can be passed to a model by providing content blocks.
                - when returning multimodal data as part of the response the resulting `AIMessage` will have content blocks with multimodal types
            - Reasoning
                - perform multi-step reasoning to arrive at a conclusion.
                - involves breaking down complex problems into smaller, more manageable steps
                - Depending on the model
                    - surface this reasoning process
                    - specify the level of effort it should put into reasoning
                    - request that the model turn off reasoning entirely
        - can be utilized in two ways:
            1. With agents
                - specified when creating an agent
            2. Standalone
                - called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.
        - key methods
            1. Invoke
                - The model takes messages as input and outputs messages after generating a complete response.
            2. Stream
                - Invoke the model, but stream the output as it is generated in real-time.
            3. Batch
                - Send multiple requests to a model in a batch for more efficient processing.
                - By default, `batch()` will only return the final output for the entire batch.
                    - to receive the output for each individual input as it finishes generating, stream results with `batch_as_completed()`
                        - results may arrive out of order. Each includes the input index
        - LangChain supports running models locally on your own hardware
            - Ollama is one of the easiest ways to run models locally
        - Many providers offer prompt caching features
            - Prompt caching is often only engaged above a minimum input token threshold
        - server-side tool-calling loops
            - models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn
        - Many chat model providers impose a limit on the number of invocations that can be made in a given time period.
        - For many chat model integrations, you can configure the base URL for API requests
        - Certain models can be configured to return token-level log probabilities representing the likelihood of a given token
        - A number of model providers return token usage information as part of the invocation response.
        - When invoking a model, additional configuration can be passed through the config parameter
    - [x] https://docs.langchain.com/oss/python/langchain/messages
        - the fundamental unit of context for models in LangChain
        - represent the input and output of models, carrying both the content and metadata
        - Messages are objects that contain:
            1. Role
                - the message type (e.g. system, user)
            2. Content
                - content of the message (like text, images, audio, documents, etc.)
            3. Metadata
                - Optional fields such as response information, message IDs, and token usage
        - formats
            1. Text prompts (strings)
                - ideal for straightforward generation tasks where retaining conversation history is not needed.
            2. list of message objects
            3. dictionary format (OpenAI chat completions format)
        - Message types
            1. System message
                - Tells the model how to behave and provide context for interactions
                - set the tone, define the model’s role, and establish guidelines for responses
            2. Human message
                - Represents user input and interactions with the model
            3. AI message
                - Responses generated by the model, including text content, tool calls, and metadata
                - sometimes helpful to manually create a new `AIMessage` object and insert it into the message history as if it came from the model (to affect message's weight)
                - the tool calls will be included in the `AIMessage` (`tool_calls`)
                - can hold token counts and other usage metadata in its usage_metadata field
            4. Tool message
                - Represents the outputs of tool calls
        - `content` attribute is loosely-typed, supporting strings and lists of untyped objects
            - LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content.
        - Message objects implement a content_blocks property that will lazily parse the content attribute into a standard, type-safe representation.
            - each item in the list must adhere to one of the listed block types https://docs.langchain.com/oss/python/langchain/messages#content-block-reference
                - TextContentBlock, ReasoningContentBlock, ImageContentBlock, AudioContentBlock, VideoContentBlock, FileContentBlock, PlainTextContentBlock, ToolCall, ToolCallChunk, InvalidToolCall, ServerToolCall, ServerToolCallChunk, ServerToolResult, NonStandardContentBlock
        - can opt-in to storing content blocks in message content
        - chat models accept a sequence of message objects as input and return an `AIMessage` as output.
    - [x] https://docs.langchain.com/oss/python/langchain/tools
        - let agents fetch real-time data, execute code, query external databases, and take actions in the world.
        - The model decides when to invoke a tool based on the conversation context, and what input arguments to provide
        - created using `@tool` decorator
            - by default, the tool name comes from the function name. It can be overriden using first positional parameter
            - the auto-generated tool description can be overriddent using `description` parameter
            - complex inputs are defined using `args_schema` parameter
                - names config and runtime cannot be used as tool argumets
            - can access runtime information through the ToolRuntime parameter
                - add `runtime: ToolRuntime` to the tool signature
                - `ToolRuntime` provides:
                    - State - Mutable data that flows through execution (e.g., messages, counters, custom fields)
                    - Context - Immutable configuration like user IDs, session details, or application-specific configuration
                    - Store - Persistent long-term memory across conversations
                    - Stream Writer - Stream custom updates as tools execute
                        - If `runtime.stream_writer` is used inside a tool, the tool must be invoked within a LangGraph execution context
                    - Config - RunnableConfig for the execution
                    - Tool Call ID - ID of the current tool call
    - [x] https://docs.langchain.com/oss/python/langchain/short-term-memory
        - lets application remember previous interactions within a single conversation.
        - to use short term memory you need to specify a checkpointer when creating an agent
            - in production, use a checkpointer backed by a database
        - by default, agents use `AgentState` via a messages key
        - common patterns
            1. Trim messages
                - count the tokens in the message history and truncate whenever it approaches that limit
                    - in LangChain, can use the trim messages utility using the `@before_model` middleware
            2. Delete messages
                - use the `RemoveMessage`
                - need to use a state key with `add_messages` reducer
                - Check the limitations of the LLM provider you’re using
                    - ex. Some providers expect message history to start with a user message
            3. Summarize messages
                - The problem with trimming or removing messages, is that information may be lost
                - To summarize message history, use the built-in `SummarizationMiddleware`
        - accessing memory
            - in tools
                - read using the `ToolRuntime` parameter
                - To modify during execution, you can return state updates directly from the tools.
            - in `@dynamic_prompt` middleware to create dynamic prompts
                - using `request: ModelRequest` attribute
            - in `@before_model` middleware to process messages before model calls
                - using `state: AgentState` attribute
            - in `@after_model` middleware to process messages after model calls
                - using `state: AgentState` attribute
    - [x] https://docs.langchain.com/oss/python/langchain/streaming
        - displaying output progressively before a complete response is ready
            - Stream agent progress
                — get state updates after each agent step.
                - use the `stream` or `astream` methods with `stream_mode="updates"`
            - Stream LLM tokens
                — stream language model tokens as they’re generated.
                - use the `stream` or `astream` methods with `stream_mode="messages"`
            - Stream custom updates
                — emit user-defined signals (e.g., "Fetched 10/100 records").
                - use `get_stream_writer`
            - Stream multiple modes
                — choose from updates (agent progress), messages (LLM tokens + metadata), or custom (arbitrary user data).
                - use the `stream` or `astream` methods with `stream_mode` as a list
        - stream modes (can use 1 or more)
            1. updates
                - Streams state updates after each agent step. Parallel updates are streamed separately.
            2. messages
                - Streams tuples of (token, metadata) from any graph nodes where an LLM is invoked.
            3. custom
                - Streams custom data from inside your graph nodes using the stream writer.
        - common patterns
            1. stream tool calls
                - To access the completed messages with parsed tool calls:
                    - If those messages are tracked in the state
                        - use `stream_mode=["messages", "updates"]` to access completed messages through state updates.
                    - If those messages are not tracked in the state
                        - use custom updates or aggregate the chunks during the streaming loop.
            2. streaming with human-in-the-loop
                1. configure the agent with human-in-the-loop middleware and a checkpointer
                2. collect interrupts generated during the "updates" stream mode
                3. respond to those interrupts with a command
                4. collect a decision for each interrupt
            3. Streaming from sub-agents
                - to disambiguate the source of messages as they are generated when there are multiple LLMs
                    - initialize any model with tags
                    - these tags are then available in metadata
                    - specify `subgraphs=True` when creating the stream
        - to disable streaming of individual tokens for a given model set `streaming=False` when initializing the model.
    - [x] https://docs.langchain.com/oss/python/langchain/middleware/overview
        - Middleware provides a way to control what happens inside the agent
        - Middleware exposes hooks before and after each of the angent steps: starting, calling a model, choosing tools, finishing
    - [x] https://docs.langchain.com/oss/python/langchain/middleware/built-in
            - Summarization
                - Automatically summarize conversation history when approaching token limits.
            - Human-in-the-loop
                - Pause execution for human approval of tool calls.
            - Model call limit
                - Limit the number of model calls to prevent excessive costs.
            - Tool call limit
                - Control tool execution by limiting call counts.
            - Model fallback
                - Automatically fallback to alternative models when primary fails.
            - PII detection
                - Detect and handle Personally Identifiable Information (PII).
            - To-do list
                - Equip agents with task planning and tracking capabilities.
            - LLM tool selector
                - Use an LLM to select relevant tools before calling main model.
            - Tool retry
                - Automatically retry failed tool calls with exponential backoff.
            - Model retry
                - Automatically retry failed model calls with exponential backoff.
            - LLM tool emulator
                - Emulate tool execution using an LLM for testing purposes.
            - Context editing
                - Manage conversation context by trimming or clearing tool uses.
            - Shell tool
                - Expose a persistent shell session to agents for command execution.
            - File search
                - Provide Glob and Grep search tools over filesystem files.
