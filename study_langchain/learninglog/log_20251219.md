- langchain
    - [x] https://docs.langchain.com/oss/python/langchain/overview
        - building agents and applications powered by LLMs
        - a pre-built agent architecture and model integrations
        - built on top of LangGraph
        - use LangGraph, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.
    - [x] https://docs.langchain.com/oss/python/langchain/quickstart
        - preparation
            - Install the LangChain package
            - Set up a Claude (Anthropic) account and get an API key
            - Set the ANTHROPIC_API_KEY environment variable in your terminal
        - basic agent example
        - weather forecasting example
            1. define system prompt
            2. create tools
            3. set up language model
            4. define a structured response format
            5. add memory
            6. create and run the agent
        - Tools let a model interact with external systems by calling defined functions.
            - can depend on runtime context
            - can interact with agent memory
            - should be well-documented: their name, description, and argument names become part of the model’s prompt.
        - Memory allows the agent to remember previous conversations and context
    - [x] https://docs.langchain.com/oss/python/langchain/component-architecture
        - page contains helpful diagrams
            - core component ecosystem
                - component layers
                    - Input processing: Text input, Document loaders, Text splitters, Documents
                    - Embedding & storage: Embedding models, Vectors, Vector Stores
                    - Retrieval: User Query, Embedding models, Query vector, Retrievers, Relevant context
                    - Generation: Chat models, AI response, Tools, Tool results
                    - Orchestration: Agents, Memory
            - common patterns
                - RAG
                - Agent with tools
                - Multi-agent system
        - main component categories: Document Processing, Models, Vector Stores, Tools, Retrievers, Agents, Memory
- concepts
    - [x] https://docs.langchain.com/oss/python/concepts/memory
        - lets remember previous interactions, learn from feedback, and adapt to user preferences
        - two memory types
            1. Short-term memory, or thread-scoped memory
                - maintains message history within a session as a part of the agent’s `state`
                - maintains separation between different threads
                - persisted to a database using a `checkpointer`
                - updates when the graph is invoked or a step is completed
                - is read at the start of each step
                - "Conversation history" is the most common form
                    - A full history may not fit inside an LLM’s context window, resulting in an irrecoverable error
                    - can benefit from using techniques to manually remove or forget stale information
            2. Long-term memory
                - can be recalled at any time and in any thread
                - LangGraph provides `stores` to let you save and recall long-term memories
                - is saved within custom “namespaces.”
                - questions to help navigate the different techniques
                    - What is the type of memory? 
                        - facts (semantic memory)
                        - experiences (episodic memory)
                        - rules (procedural memory)
                    - When do you want to update memories?
                        - as part of an agent’s application logic
                        - as a background task
                - Semantic memory
                    - involves the retention of specific facts and concepts.
                    - the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions
                    - can be a single, continuously updated “profile” of well-scoped and specific information
                        - generally just a JSON document with various key-value pairs
                        - may benefit from splitting a profile into multiple documents or strict decoding
                    - can be a collection of documents that are continuously updated and extended
                        - individual memory can be more narrowly scoped and easier to generate
                        - model must delete or update existing items in the list
                            - some models may default to over-inserting and others may default to over-updating
                        - shifts complexity to memory search over the list
                        - can make it challenging to provide comprehensive context to the model
                - Episodic memory
                    - involves recalling past events or actions
                    - used to help an agent remember how to accomplish a task
                    - are often implemented through few-shot example prompting
                    - LLMs learn well from examples
                    - the challenge lies in selecting the most relevant examples based on user input
                - Procedural memory
                    - involves remembering the rules used to perform tasks
                    - a combination of model weights, agent code, and agent’s prompt that collectively determine the agent’s functionality
                    - Reflection or meta-prompting
                        1. prompting the agent with its current instructions along with recent conversations or explicit user feedback.
                        2. The agent then refines its own instructions based on this input.
                - Writing memories
                    1. in the hot path
                        - real-time updates; new memories are immediately available for use in subsequent interactions
                        - users can be notified when memories are created and stored
                        - may increase complexity if the agent requires a new tool to decide what to commit to memory
                        - can impact agent latency
                        - the agent must multitask between memory creation and its other responsibilities
                    2. in the background
                        - eliminates latency in the primary application
                        - separates application logic from memory management
                        - allows for more focused task completion by the agent
                        - flexibility in timing memory creation
                        - infrequent updates may leave other threads without new context
                - Memory storage
                    - LangGraph stores long-term memories as JSON documents in a store
                    - Each memory is organized under a custom namespace and a distinct key
                    - enables hierarchical organization of memories
                    - Cross-namespace searching is supported through content filters
    - [x] https://docs.langchain.com/oss/python/concepts/context
        - building dynamic systems that provide the right information and tools, in the right format
        - By mutability
            - Static context
            - Dynamic context
        - By lifetime
            - Runtime context
            - Cross-conversation context
        - Runtime context - local context: data and dependencies the code needs to run
            - is a form of dependency injection (like database connections, user IDs, or API clients)
        - often confused with
            - The LLM context - the data passed into the LLM’s prompt.
            - The “context window” - the maximum number of tokens that can be passed to the LLM.
        - ways to manage context
            - Static runtime context
                - immutable data like user metadata, tools, and database connections
                - passed to an application at the start of a run via the context argument to invoke/stream
            - Dynamic runtime context
                - mutable data that can evolve during a single run and is managed through the LangGraph state object
                - includes conversation history, intermediate results, and values derived from tools or LLM outputs
            - Dynamic cross-conversation context
                - persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store
                - user profiles, preferences, and historical interactions
                - can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).
- langgraph
    - [x] https://docs.langchain.com/oss/python/langgraph/overview
        - low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents
        - focused entirely on agent orchestration
        - provides low-level supporting infrastructure for any long-running, stateful workflow or agent
            - Durable execution
                - persist through failures and can run for extended periods, resuming from where they left off.
            - Human-in-the-loop
                - inspecting and modifying agent state at any point
            - Comprehensive memory
                - both short-term working memory for ongoing reasoning and long-term memory across sessions.
            - Debugging with LangSmith
                - trace execution paths, capture state transitions, and provide detailed runtime metrics.
            - Production-ready deployment
                - scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.
    - [x] https://docs.langchain.com/oss/python/langgraph/graph-api
        - LangGraph models agent workflows as graphs
        - key components:
            1. State
                - current snapshot of your application
                - consists of
                    - the `Schema` of the graph
                        - can be either a TypedDict or a Pydantic model
                            - dataclass allows providing default values in your state
                            - Pydantic gives recursive data validation but is less performant than a TypedDict or dataclass
                        - by default, same input and output schemas
                            - can specify explicit input and output schemas directly
                                - we define an “internal” schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the “internal” schema to constrain the input and output of the graph.
                        - nodes can write to private state channels inside the graph(a private schema, PrivateState).
                        - node can write to any state channel in the graph state
                    - reducer functions
                        - specify how to apply updates to the state
                        - Each key in the State has its own independent reducer function
                        - types of reducers
                            - default reducer
                            - operator.add, etc.
                            - Overwrite
                - it is helpful to store prior conversation history as a list of messages in the graph state
                    - by default every state update will overwrite the list of messages with the most recently provided value.
                    - to simply append messages to the existing list, you could use operator.add as a reducer.
                    - the prebuilt add_messages function for brand new messages will simply append to existing list, but it will also handle the updates for existing messages correctly.
                        - will also try to deserialize messages into LangChain Message objects
                    - a prebuilt state called MessagesState makes it easy to use messages.
                        - MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer.
            2. Nodes
                - Functions - the logic of the agents
                    - either synchronous or asynchronous
                    - accepts the following arguments: state, RunnableConfig, Runtime
                - receive the current state, return an updated state.
                - node state
                    - (At the start of graph execution) - "inactive" state.
                    - (receives a new message on one of its incoming edges) - "active" state.
                    - (At the end of a super-step node with no incoming messages) - "inactive" state.
                    - The graph execution terminates when all nodes are inactive and no messages are in transit.
                - START Node
                    - a special node that represents the node that sends user input to the graph.
                - END Node
                    - a special node that represents a terminal node
                    - to denote which edges have no actions after they are done
                - To use caching:
                    - Specify a cache when compiling a graph (or specifying an entrypoint)
                    - Specify a cache policy for nodes.
                        - key_func (default - hash)
                        - ttl (in seconds, default - no expire)
            3. Edges
                - determine which Node to execute
                - how the logic is routed
                - how the graph decides to stop
                - Normal Edges
                    - Go directly from one node to the next.
                - Conditional Edges
                    - Call a function to determine which node(s) to go to next.
                - Entry Point
                    - Which node(s) to call first when user input arrives.
                - Conditional Entry Point
                    - Call a function to determine which node(s) to call first when user input arrives.
                - If a node has multiple outgoing edges, all will be executed in parallel as a part of the next superstep.
        - uses message passing to define a general program
        - When a Node completes its operation, it sends messages along one or more edges to other node(s)
        - A super-step can be considered a single iteration over the graph nodes
            - nodes that run in parallel => part of the same super-step
            - nodes that run sequentially => belong to separate super-steps
        - To build a graph
            1. define the state
            2. add nodes and edges
            3. compile it
        - By default, Nodes and Edges operate on the same shared state
            - But LangGraph supports returning `Send` objects from conditional edges. Send takes two arguments: the name of the node, and the state to pass to that node.
        - returning a Command object allows to both perform state updates and decide which node to go to next in the same node
            - In such case you must add return type annotations with the list of node names the node is routing to
            - it's possible to navigate to a node in the closest parent using Command and `graph=Command.PARENT`
            - used for updating graph state from inside a tool
            - is an important part of human-in-the-loop workflows
        - LangGraph can easily handle migrations of graph definitions (nodes, edges, and state)
        - context_schema(StateGraph) is used to specify a for runtime context passed to nodes
            - can pass this context into the graph using the context parameter of the invoke method.
        - The recursion limit sets the maximum number of super-steps the graph can execute during a single execution.
            - LangGraph will raise `GraphRecursionError`
            - By default this value is set to 25 steps
            - The current step counter is accessible in config["metadata"]["langgraph_step"]
        - LangGraph comes with several built-in ways to visualize graphs
    - [x] https://docs.langchain.com/oss/python/langgraph/functional-api
        - The Functional API allows adding LangGraph’s key features — persistence, memory, human-in-the-loop, and streaming — to existing applications with minimal changes
        - Graph vs Functional APIs
            - Control flow
                - The Functional API does not require thinking about graph structure
                    - usually less code to write
            - Short-term memory
                - The GraphAPI
                    - requires declaring a State
                    - may require defining reducers to manage updates to the graph state
                - @entrypoint and @tasks do not require explicit state management
                    - state is scoped to the function and is not shared across functions.
            - Checkpointing
                - In the Graph API a new checkpoint is generated after every superstep.
                - In the Functional API, tasks' results are saved to an existing checkpoint associated with the given entrypoint.
            - Visualization
                - The Graph API makes it easy to visualize the workflow.
                - The Functional API does not support visualization.
        - uses two key building blocks:
            1. @entrypoint
                – the starting point of a workflow
                - The function must accept a single positional argument, which serves as the workflow input.
                - produces a Pregel instance which helps to manage the execution of the workflow
                    - can be executed using the invoke, ainvoke, stream, and astream methods
                - pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop
                - The inputs and outputs of entrypoints must be JSON-serializable to support checkpointing
                - When declaring an entrypoint, you can request access to additional parameters that will be injected automatically at run time (e.g. previous, store, writer, config)
                - To resume execution after an interrupt
                    - pass a resume value to the Command primitive.
                - To resume execution after an error (assumes error has been resolved)
                    - run the entrypoint with a None and the same thread id (config).
                - defining entrypoint with a checkpointer allows storing information between successive invocations on the same thread id in checkpoints and accessing it in next invocation using `previous` parameter.
                - entrypoint.final is a primitive that allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.
            2. @task
                – a discrete unit of work, such as an API call or data processing step
                - key characteristics:
                    - Asynchronous Execution
                        - designed to be executed asynchronously.
                    - Checkpointing
                        - Task results are saved to a checkpoint.
                - The outputs of tasks must be JSON-serializable to support checkpointing.
                - can only be called from within an entrypoint, another task, or a state graph node.
                - provide a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.
                - provide a way to encapsulate and manage the retry logic
                - To utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks.
                - LangGraph persists task and subgraph results as they execute.
                - Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution.
                - Encapsulate side effects (e.g., writing to a file, sending an email) in tasks
                - Operations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.
