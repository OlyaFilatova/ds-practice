- [x] https://hadoop.apache.org/
    - The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.
    - designed to detect and handle failures at the application layer
    - designed to scale up from single servers to thousands of machines
    - modules:
        - Hadoop Common
            - The common utilities that support the other Hadoop modules.
        - Hadoop Distributed File System (HDFSâ„¢)
            - A distributed file system that provides high-throughput access to application data.
        - Hadoop YARN
            - A framework for job scheduling and cluster resource management.
        - Hadoop MapReduce
            - A YARN-based system for parallel processing of large data sets.
- [x] https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html
    - all production Hadoop clusters use Kerberos to authenticate callers
    - GNU/Linux is supported as a development and production platform
    - Required software for Linux include:
        - Java
        - ssh
    - supported modes:
        - Local (Standalone) Mode
            - single Java process
        - Pseudo-Distributed Mode
            - each Hadoop daemon runs in a separate Java process
        - Fully-Distributed Mode
- [x] https://cwiki.apache.org/confluence/display/HADOOP2/ImportantConcepts
    - Hadoop
        - system that runs jobs, distributes tasks (pieces of these jobs) and stores data in a parallel and distributed fashion.
    - Map/reduce
        - Is the style in which most programs running on Hadoop are written.
            - map: input is broken in tiny pieces which are processed independently
            - reduce: The results of these processes are collated into groups and processed as groups
    - Job
        - the combination of all of the JAR files and classes needed to run a map/reduce program
            - collected into a JAR which is usually referred to as a job file
        - to execute a job, you submit it to a JobTracker
    - Task
        - the program that executes the individual map and reduce steps
        - executed on TaskTracker nodes chosen by the JobTracker
    - HDFS
        - Hadoop Distributed File System.
        - This is how input and output files of Hadoop programs are normally stored
        - provides very high input and output speeds
        - provides very high bandwidth by storing chunks of files scattered throughout the Hadoop cluster
        - files are stored in multiple places, tasks are placed near their input data and output data is largely stored where it is created
- [x] https://cwiki.apache.org/confluence/display/HADOOP2/ProjectDescription
    - Map/Reduce
        - a programming paradigm that expresses a large distributed computation as a sequence of distributed operations on data sets of key/value pairs.
    - The Hadoop Map/Reduce framework harnesses a cluster of machines and executes user defined Map/Reduce jobs across the nodes in the cluster.
        - The input to the computation is a data set of key/value pairs.
        - map:
            - many map tasks are distributed across the cluster of nodes on which the framework operates
            - splits the input data set into a large number of fragments and assigns each fragment to a map task.
            - Each map task
                - consumes key/value pairs from its assigned fragment
                - produces a set of intermediate key/value pairs.
                - invokes a user defined map function
        - the framework
            - sorts the intermediate data set by key
            - produces a set of tuples so that all the values associated with a particular key appear together
        reduce:
            - many reduce tasks are distributes across the cluster of nodes
            - each reduce task consumes the fragment of (K',V'*) tuples assigned to it.
            - invokes a user-defined reduce function

        - if node(s) fail in the middle of a computation the tasks assigned to them are re-distributed among the remaining nodes
        - a single master server or jobtracker and several slave servers or tasktrackers, one per node in the cluster.
    - Hadoop DFS stores each file as a sequence of blocks, all blocks in a file except the last block are the same size.
        - Blocks are replicated for fault tolerance.
        - The block size and replication factor are configurable per file
        - consists of
            - a single Namenode
                - a master server that manages the filesystem namespace and regulates access to files by clients
                - makes filesystem namespace operations available via an RPC interface
                - determines the mapping of blocks to Datanodes
            - Datanodes
                - one per node in the cluster
                - manage storage attached to the nodes that they run on
                - responsible for serving read and write requests from filesystem clients
                - perform block creation, deletion, and replication upon instruction from the Namenode
- [x] https://cwiki.apache.org/confluence/display/HADOOP2/HadoopIsNot
    - is NOT a substitute for a database
        - Hadoop works where the data is too big for a database (the technical limits)
            - With very large datasets, the cost of regenerating indexes is so high you can't easily index changing data
            - With many machines trying to write to the database, you can't get locks on it
    - HDFS is not a complete POSIX filesystem
        - e.g. you can only add data to the end of the file, not seek to the middle and write things.
